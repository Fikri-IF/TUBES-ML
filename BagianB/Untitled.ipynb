{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from layer import Layer\n",
    "import util\n",
    "\n",
    "class ANN:\n",
    "    def __init__(self, learningRate, errorThreshold, maxIter, batchSize, countLayer):\n",
    "        self.learningRate = learningRate\n",
    "        self.countLayer=countLayer\n",
    "        self.layers = []\n",
    "        self.iteration= None\n",
    "        self.errorThreshold = errorThreshold\n",
    "        self.maxIter = maxIter\n",
    "        self.batchSize = batchSize\n",
    "\n",
    "    def addLayer(self, neuronTotal, activationFunction, weight, bias):\n",
    "        self.layers.append(Layer(neuronTotal, activationFunction, bias))\n",
    "        if (len(self.layers) > 1):\n",
    "            # set weight untuk layer selain input layer\n",
    "            self.bias=bias\n",
    "            self.layers[-1].setWeights(weight)\n",
    "    \n",
    "    def forwardPropagation(self, input):\n",
    "        for i in range(len(self.layers)):\n",
    "            if (i == 0):\n",
    "                self.layers[i].setOutput(input, True)\n",
    "                continue\n",
    "            result = np.dot(self.layers[i-1].output, self.layers[i].weights)\n",
    "            self.layers[i].setOutput(self.layers[i].bias + result)\n",
    "            print(self.layers[i].output)\n",
    "\n",
    "        pred = np.argmax(self.layers[-1].output, axis = 1)\n",
    "        return np.reshape(pred, (pred.shape[0],1))\n",
    "    \n",
    "    def backwardPropagation(self, prediction):\n",
    "        dE_dOut = 0\n",
    "        val = 0\n",
    "        for i in range (1,len(self.layers)):            \n",
    "            if (self.layers[i].activationFunction == \"softmax\"): # softmax menggunakan cross entropy\n",
    "                dE_dOut = util.difCrossEntropy(prediction, self.layers[-1].output)\n",
    "                # print(\"de\",dE_dOut)\n",
    "                val = dE_dOut\n",
    "            else:\n",
    "                dE_dOut = util.difSse(prediction, self.layers[-1].output)\n",
    "                # if (i == 1):\n",
    "                #     print(\"de1\",dE_dOut)\n",
    "                val = dE_dOut\n",
    "\n",
    "            diffOut = 0\n",
    "\n",
    "            for j in range(len(self.layers)-1-i):\n",
    "                if (self.layers[1-j].activationFunction == \"softmax\"):\n",
    "                    diffOut = util.difSoftmax(self.layers[-1-j].input)\n",
    "                    val = val * diffOut\n",
    "                    val = np.dot(val, self.layers[-1-j].weights.T)\n",
    "                elif (self.layers[1-j].activationFunction == \"relu\"):\n",
    "                    diffOut = util.difRelu(self.layers[-1-j].input)\n",
    "                    val = val * diffOut\n",
    "                    val = np.dot(val, self.layers[-1-j].weights.T)\n",
    "                elif (self.layers[1-j].activationFunction == \"linear\"):\n",
    "                    diffOut = util.difLinear(self.layers[-1-j].input)                                        \n",
    "                    val = val * diffOut\n",
    "                    val = np.dot(val, self.layers[-1-j].weights.T)\n",
    "                elif (self.layers[1-j].activationFunction == \"sigmoid\"):\n",
    "                    diffOut = util.difSigmoid(self.layers[-1-j].input)                                        \n",
    "                    val = val * diffOut\n",
    "                    val = np.dot(val, self.layers[-1-j].weights.T)\n",
    "                else:\n",
    "                    quit()\n",
    "\n",
    "            index = -1- (len(self.layers)-1-i)\n",
    "            if (self.layers[index].activationFunction == \"softmax\"):\n",
    "                diffOut = util.difSoftmax(self.layers[index].input)\n",
    "                val = val * diffOut\n",
    "            elif (self.layers[index].activationFunction==\"relu\"):\n",
    "                diffOut = util.difRelu(self.layers[index].input)\n",
    "                val = val * diffOut\n",
    "            elif (self.layers[index].activationFunction==\"linear\"):\n",
    "                diffOut = util.difLinear(self.layers[index].input)                                        \n",
    "                val = val * diffOut\n",
    "            elif (self.layers[index].activationFunction==\"sigmoid\"):\n",
    "                diffOut = util.difSigmoid(self.layers[index].input)                                        \n",
    "                val = val * diffOut\n",
    "            else:\n",
    "                quit()\n",
    "            dInput = self.layers[-2- index].output\n",
    "            self.layers[i].weights = self.layers[i].weights - (self.learningRate * np.dot(np.array(dInput).T,val))\n",
    "            self.layers[i].bias  = self.layers[i].bias - (self.learningRate * np.mean(np.dot(np.array(dInput).T, val)))\n",
    "    \n",
    "    def train(self, x_train, y_train):\n",
    "        cumulativeError = float('inf')\n",
    "        totalEpoch = int(len(x_train) / self.batchSize)\n",
    "        for epoch in range(totalEpoch): \n",
    "            for batch in range(0, len(x_train), self.batchSize):\n",
    "                x_batch = x_train[batch:batch+self.batchSize]\n",
    "                y_batch = y_train[batch:batch+self.batchSize]\n",
    "\n",
    "                # feed forward\n",
    "                prediction = self.forwardPropagation(x_batch)\n",
    "\n",
    "                # backward dan update bobot\n",
    "                self.backwardPropagation(prediction)\n",
    "\n",
    "                e = np.mean(util.sse(y_batch, prediction)).mean()\n",
    "                cumulativeError += e\n",
    "            if cumulativeError <= self.errorThreshold or epoch == totalEpoch-1 or batch == self.maxIter:\n",
    "                print(\"Training stopped at epoch:\", epoch+1, \"with cumulative error:\", cumulativeError)\n",
    "                break\n",
    "            cumulativeError = 0.0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import util\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, neuronTotal, activationFunction, bias):\n",
    "        self.neuronTotal = neuronTotal\n",
    "        if (activationFunction is not None):\n",
    "            self.activationFunction = activationFunction.lower()\n",
    "        else:\n",
    "            self.activationFunction = activationFunction\n",
    "        self.weights = None\n",
    "        self.bias = bias\n",
    "        self.output = None\n",
    "\n",
    "    def setWeights(self, weights):\n",
    "        self.weights = np.array(weights)\n",
    "\n",
    "    def setOutput(self, input, isInputLayer = False):\n",
    "        self.input = input\n",
    "        if (isInputLayer):\n",
    "            self.output = input\n",
    "        else:\n",
    "            if (self.activationFunction == \"linear\"):\n",
    "                self.output = util.linear(input)\n",
    "            elif (self.activationFunction == \"sigmoid\"):\n",
    "                self.output = util.sigmoid(input)\n",
    "            elif (self.activationFunction == \"relu\"):\n",
    "                self.output = util.relu(input)\n",
    "            elif (self.activationFunction == \"softmax\"):\n",
    "                self.output = util.softmax(input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
